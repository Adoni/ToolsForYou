{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 28.772s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Although I realize that principle is not one of your strongest\\npoints, I would still like to know why do do not ask any question\\nof this sort about the Arab countries.\\n\\n   If you want to continue this think tank charade of yours, your\\nfixation on Israel must stop.  You might have to start asking the\\nsame sort of questions of Arab countries as well.  You realize it\\nwould not work, as the Arab countries' treatment of Jews over the\\nlast several decades is so bad that your fixation on Israel would\\nbegin to look like the biased attack that it is.\\n\\n   Everyone in this group recognizes that your stupid 'Center for\\nPolicy Research' is nothing more than a fancy name for some bigot\\nwho hates Israel.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples=[]\n",
    "for line in open('./wechat.txt'):\n",
    "    data_samples.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 86.025s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features)\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "tf_feature_names=dict(zip(tf_feature_names,range(len(tf_feature_names))))\n",
    "fout1=open('./transfered_data_text.txt','w')\n",
    "fout2=open('./transfered_data_id.txt','w')\n",
    "fout3=open('./transfered_data_libsvm.txt','w')\n",
    "fout4=open('./feature_names.txt','w')\n",
    "fout4.write('\\n'.join(tf_feature_names.keys()))\n",
    "for i,d in enumerate(data_samples):\n",
    "    d1=list(filter(lambda w:w in tf_feature_names,d.split(' ')))\n",
    "    fout1.write(' '.join(d1)+'\\n')\n",
    "    d2=list(map(lambda w:str(tf_feature_names[w]),d1))\n",
    "    fout2.write(' '.join(d2)+'\\n')\n",
    "    d3=Counter(d2)\n",
    "    d3=list(map(lambda v:'%s:%d'%(v),d3.items()))\n",
    "    fout3.write(' '.join(d3)+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 181.309s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: 健康 身体 作用 分钟 疾病 皮肤 治疗 患者 食物 时间 人体 医生 宝宝 效果 方法 营养 运动 功能 导致 功效\n",
      "Topic #1: 工作 公司 学习 时间 管理 员工 能力 客户 企业 团队 经验 课程 老板 专业 成功 老师 销售 方法 方式 事情\n",
      "Topic #2: 关注 二维码 识别 女人 免费 简介 微信 订阅 男人 点击 生活 健康 分享 选择 添加 id 轻松 技巧 搭配 公众\n",
      "Topic #3: 产品 用户 技术 数据 公司 平台 互联网 手机 行业 市场 品牌 企业 服务 中国 发展 系统 智能 科技 网络 模式\n",
      "Topic #4: 微信 点击 阅读 浏览器 播放 回复 原文 音乐 支持 文章 朋友 查看 视频 公众 内容 语音 时间 关注 手机 下方\n",
      "Topic #5: 公司 市场 投资 汽车 银行 基金 资金 风险 上市 金融 价格 车型 股票 股份 资产 交易 业务 机构 行业 证券\n",
      "Topic #6: 设计 时间 生活 电影 搭配 世界 活动 空间 选择 喜欢 中国 品牌 第一 酒店 风格 推荐 地址 时尚 感觉 自然\n",
      "Topic #7: 孩子 生活 女人 男人 喜欢 父母 朋友 妈妈 人生 时间 事情 家庭 东西 发现 世界 幸福 工作 儿子 第一 感觉\n",
      "Topic #8: 中国 美国 经济 国家 城市 投资 发展 市场 企业 韩国 北京 政府 政策 房地产 房价 金融 日本 公司 乐天 社会\n",
      "Topic #9: 工作 服务 建设 管理 人员 项目 发展 国家 信息 全国 交通 单位 企业 工程 中心 相关 有限公司 招聘 标准 部门\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
